{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import nltk.data\n",
    "tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "\n",
    "from tqdm import tqdm\n",
    "tqdm.pandas(desc=\"progress-bar\")\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "import gensim\n",
    "from gensim.models.word2vec import Word2Vec # the word2vec model gensim class\n",
    "LabeledSentence = gensim.models.doc2vec.LabeledSentence\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "stop = stopwords.words('english')\n",
    "\n",
    "from numpy import asarray\n",
    "from collections import Counter\n",
    "import string\n",
    "from string import punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>headline</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>MMM fell on hard times but could be set to reb...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Wolfe Research Upgrades 3M MMM to ¡§Peer Perfo...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3M MMM Upgraded to ¡§Peer Perform¡¨ by Wolfe R...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>MMM insideday follow up as it also opened up w...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>MMM is best dividend stock out there and down ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>MMM  3M The Fallen Dividend King Will Be Back ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>MMMcelebrates New Year with 7 month high close...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>MMM above 180 baby is going higher</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>MMMhasnt really done much this year but it loo...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>3M MMM Rating Increased to Neutral at JPMorgan...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>3M MMM Stock Rating Upgraded by JPMorgan Chase...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>3M MMM Upgraded to Neutral by JPMorgan Chase  Co</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>MMM higher higher in move above both long term...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Highlight MMM is one of the bestperforming Dow...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>cina tairff cut will help MMM</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>MMM gets upgrade from TUSA JPM</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>3M upgrade Valeritas downgrades among todays t...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>JPMorgans Tusa Upgrades 3M On Neutral On More ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>Trading Ideas MMM Bullish upgrade from JP Morg...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>3M MMM Upgraded to ¡§Neutral¡¨ at JPMorgan Cha...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>JPMorgans Tusa upgrades 3M on Neutral on more ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>MMM UPGRADED TO NEUTRAL AT JPM 150 PT</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>MMM up 6 since casually mentioning it should b...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>3M MMM Cut to Sell at UBS Group</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>3M MMM Cut to ¡§Sell¡¨ at UBS Group</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>The industry average ROE is 1096 MMM outperfor...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>3M MMM Lowered to Sell at UBS Group</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>3M MMM Upgraded by Zacks Investment Research t...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>3M MMM Raised to ¡§Hold¡¨ at Zacks Investment ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>3M MMM Downgraded to ¡§Neutral¡¨ at Citigroup ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9440</th>\n",
       "      <td>WMT SAID TO EXPLORE OPTIONS FOR ASDA INCLUDING...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9441</th>\n",
       "      <td>Walmart To Explore Options For Asda Including ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9442</th>\n",
       "      <td>WMT Said to explore options for Asda unit incl...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9443</th>\n",
       "      <td>Walmart Reportedly Cited To Take Its Time To A...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9444</th>\n",
       "      <td>Walmart Is Said To Explore Options For Asda Un...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9445</th>\n",
       "      <td>WMT this had a beautiful close Headed to last ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9446</th>\n",
       "      <td>WMT will offer higher quality meat</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9447</th>\n",
       "      <td>Walmart Wants to Take On Amazon in Digital Adv...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9448</th>\n",
       "      <td>WMT SFIXWalmart Jumps on the Clothing Subscrip...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9449</th>\n",
       "      <td>Food stamps and online grocery shopping are ab...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9450</th>\n",
       "      <td>Walmart de Mexico S A B de C V WMMVY Lifted to...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9451</th>\n",
       "      <td>Shop online Ways to reduce damage to the envir...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9452</th>\n",
       "      <td>Investors Sell Shares of Walmart WMT on Streng...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9453</th>\n",
       "      <td>Investors Sell Shares of Walmart WMT on Streng...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9454</th>\n",
       "      <td>WMT beast 386 headed to new all time highs</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9455</th>\n",
       "      <td>WMT KRFDA To Retailers Do Better In Curbing Te...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9456</th>\n",
       "      <td>As Walmart INC WMT Stock Rose Cadence Capital ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9457</th>\n",
       "      <td>Wal WMT Share Value Rose While Montag A  Assoc...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9458</th>\n",
       "      <td>WMT CASYFines threatened for Walmart Kroger ot...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9459</th>\n",
       "      <td>Zacks Investment Research Downgrades Walmart d...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9460</th>\n",
       "      <td>Analysts take a look at Walmart Inc WMT having...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9461</th>\n",
       "      <td>RT JeffMacke WMT trying to get daylight over 1...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9462</th>\n",
       "      <td>Walmart to expand inhouse ad technology with p...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9463</th>\n",
       "      <td>WMT trying to get daylight over 100 as Tiger a...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9464</th>\n",
       "      <td>AMZN WMT TGTJeff Bezos Challenges Rivals to To...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9465</th>\n",
       "      <td>WMT  Walmart shifts to remodeling vs new stores</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9466</th>\n",
       "      <td>Walmart INC WMT Holder Texas Permanent School ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9467</th>\n",
       "      <td>WMT GILD3 Dividend Stocks Perfect for Retirees</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9468</th>\n",
       "      <td>Walmart expanding use of robots to scan shelve...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9469</th>\n",
       "      <td>WMT Walmart plans to add thousands of robot he...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>9470 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               headline  sentiment\n",
       "0     MMM fell on hard times but could be set to reb...          0\n",
       "1     Wolfe Research Upgrades 3M MMM to ¡§Peer Perfo...          1\n",
       "2     3M MMM Upgraded to ¡§Peer Perform¡¨ by Wolfe R...          1\n",
       "3     MMM insideday follow up as it also opened up w...          1\n",
       "4     MMM is best dividend stock out there and down ...          0\n",
       "5     MMM  3M The Fallen Dividend King Will Be Back ...          1\n",
       "6     MMMcelebrates New Year with 7 month high close...          1\n",
       "7                    MMM above 180 baby is going higher          1\n",
       "8     MMMhasnt really done much this year but it loo...          1\n",
       "9     3M MMM Rating Increased to Neutral at JPMorgan...          1\n",
       "10    3M MMM Stock Rating Upgraded by JPMorgan Chase...          1\n",
       "11     3M MMM Upgraded to Neutral by JPMorgan Chase  Co          1\n",
       "12    MMM higher higher in move above both long term...          1\n",
       "13    Highlight MMM is one of the bestperforming Dow...          1\n",
       "14                        cina tairff cut will help MMM          0\n",
       "15                       MMM gets upgrade from TUSA JPM          1\n",
       "16    3M upgrade Valeritas downgrades among todays t...          0\n",
       "17    JPMorgans Tusa Upgrades 3M On Neutral On More ...          1\n",
       "18    Trading Ideas MMM Bullish upgrade from JP Morg...          1\n",
       "19    3M MMM Upgraded to ¡§Neutral¡¨ at JPMorgan Cha...          1\n",
       "20    JPMorgans Tusa upgrades 3M on Neutral on more ...          1\n",
       "21                MMM UPGRADED TO NEUTRAL AT JPM 150 PT          1\n",
       "22    MMM up 6 since casually mentioning it should b...          1\n",
       "23                      3M MMM Cut to Sell at UBS Group          0\n",
       "24                  3M MMM Cut to ¡§Sell¡¨ at UBS Group          0\n",
       "25    The industry average ROE is 1096 MMM outperfor...          1\n",
       "26                  3M MMM Lowered to Sell at UBS Group          0\n",
       "27    3M MMM Upgraded by Zacks Investment Research t...          1\n",
       "28    3M MMM Raised to ¡§Hold¡¨ at Zacks Investment ...          1\n",
       "29    3M MMM Downgraded to ¡§Neutral¡¨ at Citigroup ...          0\n",
       "...                                                 ...        ...\n",
       "9440  WMT SAID TO EXPLORE OPTIONS FOR ASDA INCLUDING...          1\n",
       "9441  Walmart To Explore Options For Asda Including ...          1\n",
       "9442  WMT Said to explore options for Asda unit incl...          1\n",
       "9443  Walmart Reportedly Cited To Take Its Time To A...          1\n",
       "9444  Walmart Is Said To Explore Options For Asda Un...          1\n",
       "9445  WMT this had a beautiful close Headed to last ...          1\n",
       "9446                 WMT will offer higher quality meat          1\n",
       "9447  Walmart Wants to Take On Amazon in Digital Adv...          1\n",
       "9448  WMT SFIXWalmart Jumps on the Clothing Subscrip...          1\n",
       "9449  Food stamps and online grocery shopping are ab...          1\n",
       "9450  Walmart de Mexico S A B de C V WMMVY Lifted to...          1\n",
       "9451  Shop online Ways to reduce damage to the envir...          1\n",
       "9452  Investors Sell Shares of Walmart WMT on Streng...          0\n",
       "9453  Investors Sell Shares of Walmart WMT on Streng...          0\n",
       "9454         WMT beast 386 headed to new all time highs          1\n",
       "9455  WMT KRFDA To Retailers Do Better In Curbing Te...          1\n",
       "9456  As Walmart INC WMT Stock Rose Cadence Capital ...          0\n",
       "9457  Wal WMT Share Value Rose While Montag A  Assoc...          0\n",
       "9458  WMT CASYFines threatened for Walmart Kroger ot...          1\n",
       "9459  Zacks Investment Research Downgrades Walmart d...          0\n",
       "9460  Analysts take a look at Walmart Inc WMT having...          1\n",
       "9461  RT JeffMacke WMT trying to get daylight over 1...          1\n",
       "9462  Walmart to expand inhouse ad technology with p...          1\n",
       "9463  WMT trying to get daylight over 100 as Tiger a...          1\n",
       "9464  AMZN WMT TGTJeff Bezos Challenges Rivals to To...          1\n",
       "9465    WMT  Walmart shifts to remodeling vs new stores          1\n",
       "9466  Walmart INC WMT Holder Texas Permanent School ...          0\n",
       "9467     WMT GILD3 Dividend Stocks Perfect for Retirees          1\n",
       "9468  Walmart expanding use of robots to scan shelve...          1\n",
       "9469  WMT Walmart plans to add thousands of robot he...          1\n",
       "\n",
       "[9470 rows x 2 columns]"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv(\"Project6500.csv\",encoding= 'unicode_escape')\n",
    "data.drop(['datetime', 'ticker'], axis=1, inplace=True)\n",
    "data = data[data.sentiment.isnull() == False]\n",
    "data['sentiment'] = data['sentiment'].map(int)\n",
    "data = data[data['headline'].isnull() == False]\n",
    "#data['headline'] = data['headline'].apply(lambda x: ' '.join([item for item in x.split() if item not in stop]))\n",
    "data['headline']=data['headline'].str.split()\n",
    "table = str.maketrans('', '', punctuation)\n",
    "data['headline']= [str(w).translate(table) for w in data['headline']]\n",
    "\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(headline):\n",
    "    headline = headline.lower()\n",
    "    tokens = tokenizer.tokenize(headline)\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "progress-bar:   0%|          | 0/9470 [00:00<?, ?it/s]\u001b[A\n",
      "progress-bar:  31%|███▏      | 2965/9470 [00:00<00:00, 29619.73it/s]\u001b[A\n",
      "progress-bar:  63%|██████▎   | 6012/9470 [00:00<00:00, 29866.70it/s]\u001b[A\n",
      "progress-bar: 100%|██████████| 9470/9470 [00:00<00:00, 30117.40it/s]\u001b[A\n"
     ]
    }
   ],
   "source": [
    "#def postprocess(data, n=1000000):\n",
    "#    data = data.head(n)\n",
    "def postprocess(data):\n",
    "    data['tokens'] = data['headline'].progress_map(tokenize)  ## progress_map is a variant of the map function plus a progress bar. Handy to monitor DataFrame creations.\n",
    "  #  data = data[data.tokens != 'NC']\n",
    " #   data.reset_index(inplace=True)\n",
    " #   data.drop('index', inplace=True, axis=1)\n",
    "    return data\n",
    "\n",
    "data = postprocess(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = Counter()\n",
    "\n",
    "for item in data['headline']:\n",
    "    # split into tokens by white space\n",
    "    tokens = str(item).split()\n",
    "    # remove remaining tokens that are not alphabetic\n",
    "    tokens = [word for word in tokens if word.isalpha()]\n",
    "    # filter out stop words\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    tokens = [w for w in tokens if not w in stop_words]\n",
    "    # filter out short tokens\n",
    "    tokens = [word for word in tokens if len(word) > 1]\n",
    "    vocab.update(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10473\n"
     ]
    }
   ],
   "source": [
    "print(len(vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('stocks', 1400), ('Co', 1140), ('Stock', 1001), ('Shares', 909), ('Sells', 776), ('Inc', 771), ('Earnings', 649), ('Price', 645), ('Raised', 588), ('Target', 586), ('Group', 559), ('EPS', 551), ('IBM', 544), ('markets', 484), ('PT', 462), ('MSFT', 428), ('Dividend', 427), ('Johnson', 417), ('INTC', 413), ('Lowered', 405), ('higher', 391), ('Rating', 387), ('AAPL', 386), ('Buy', 378), ('JPM', 374), ('The', 369), ('MMM', 361), ('CSCO', 360), ('UNH', 353), ('Insider', 352), ('WMT', 342), ('PG', 330), ('United', 328), ('Technologies', 328), ('Estimates', 323), ('Quarterly', 310), ('PFE', 306), ('HD', 306), ('UTX', 305), ('MCD', 303), ('Research', 298), ('WBA', 296), ('Verizon', 293), ('AXP', 291), ('Gamble', 289), ('Investment', 288), ('JNJ', 287), ('Procter', 285), ('RT', 283), ('CAT', 282)]\n"
     ]
    }
   ],
   "source": [
    "print(vocab.most_common(50))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4772\n"
     ]
    }
   ],
   "source": [
    "min_occurane = 2\n",
    "tokens = [k for k,c in vocab.items() if c >= min_occurane]\n",
    "print(len(tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_list(lines, filename):\n",
    "    # convert lines to a single blob of text\n",
    "    data = '\\n'.join(lines)\n",
    "    # open file\n",
    "    file = open(filename, 'w')\n",
    "    # write text\n",
    "    file.write(data)\n",
    "    # close file\n",
    "    file.close()\n",
    "\n",
    "# save tokens to a vocabulary file\n",
    "save_list(tokens, 'vocab.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(np.array(data.headline),\n",
    "                                                    np.array(data.sentiment), test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7576"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 70\n"
     ]
    }
   ],
   "source": [
    "model = Word2Vec(X_train, size=100, window=5, workers=5, min_count=1)\n",
    "# summarize vocabulary size in model\n",
    "words = list(model.wv.vocab)\n",
    "print('Vocabulary size: %d' % len(words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = 'embedding_word2vec.txt.word2vec'\n",
    "model.wv.save_word2vec_format(filename, binary=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "from string import punctuation\n",
    "from os import listdir\n",
    "from numpy import array\n",
    "from numpy import asarray\n",
    "from numpy import zeros\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import Embedding\n",
    "from keras.layers.convolutional import Conv1D\n",
    "from keras.layers.convolutional import MaxPooling1D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer()\n",
    "# fit the tokenizer on the training documents\n",
    "tokenizer.fit_on_texts(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sequence encode\n",
    "encoded_docs = tokenizer.texts_to_sequences(X_train)\n",
    "# get the max length\n",
    "max_length = max([len(s.split()) for s in X_train])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xtrain = pad_sequences(encoded_docs, maxlen=max_length, padding='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sequence encode\n",
    "encoded_docs = tokenizer.texts_to_sequences(X_test)\n",
    "# pad sequences\n",
    "Xtest = pad_sequences(encoded_docs, maxlen=max_length, padding='post')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = len(tokenizer.word_index) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2vec_embedding = load_embedding('embedding_word2vec.txt.word2vec')\n",
    "# get vectors in the right order\n",
    "word2vec_embedding_vectors = get_weight_matrix(word2vec_embedding, tokenizer.word_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2vec_embedding_layer = Embedding(vocab_size, 100, weights=[word2vec_embedding_vectors],\n",
    "                            input_length=max_length, trainable=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_4\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_2 (Embedding)      (None, 30, 100)           1074200   \n",
      "_________________________________________________________________\n",
      "conv1d_1 (Conv1D)            (None, 26, 128)           64128     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_1 (MaxPooling1 (None, 13, 128)           0         \n",
      "_________________________________________________________________\n",
      "flatten_2 (Flatten)          (None, 1664)              0         \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 1)                 1665      \n",
      "=================================================================\n",
      "Total params: 1,139,993\n",
      "Trainable params: 65,793\n",
      "Non-trainable params: 1,074,200\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "model_word2vec = Sequential()\n",
    "model_word2vec.add(word2vec_embedding_layer)\n",
    "model_word2vec.add(Conv1D(filters=128, kernel_size=5, activation='relu'))\n",
    "model_word2vec.add(MaxPooling1D(pool_size=2))\n",
    "model_word2vec.add(Flatten())\n",
    "model_word2vec.add(Dense(1, activation='sigmoid'))\n",
    "print(model_word2vec.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_word2vec.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      " - 2s - loss: 0.7859 - accuracy: 0.5945\n",
      "Epoch 2/10\n",
      " - 1s - loss: 0.6731 - accuracy: 0.6080\n",
      "Epoch 3/10\n",
      " - 1s - loss: 0.6871 - accuracy: 0.6109\n",
      "Epoch 4/10\n",
      " - 1s - loss: 0.6850 - accuracy: 0.6125\n",
      "Epoch 5/10\n",
      " - 1s - loss: 0.6722 - accuracy: 0.6135\n",
      "Epoch 6/10\n",
      " - 2s - loss: 0.6573 - accuracy: 0.6131\n",
      "Epoch 7/10\n",
      " - 2s - loss: 0.6706 - accuracy: 0.6136\n",
      "Epoch 8/10\n",
      " - 2s - loss: 0.6730 - accuracy: 0.6140\n",
      "Epoch 9/10\n",
      " - 1s - loss: 0.6577 - accuracy: 0.6144\n",
      "Epoch 10/10\n",
      " - 2s - loss: 0.6895 - accuracy: 0.6147\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x1a4ace0fd0>"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_word2vec.fit(Xtrain, y_train, epochs=10, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 61.298841\n"
     ]
    }
   ],
   "source": [
    "#### evaluation\n",
    "loss, acc = model_word2vec.evaluate(Xtest, y_test, verbose=0)\n",
    "print('Test Accuracy: %f' % (acc*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_weight_matrix(embedding, vocab):\n",
    "    # total vocabulary size plus 0 for unknown words\n",
    "    vocab_size = len(vocab) + 1\n",
    "    # define weight matrix dimensions with all 0\n",
    "    weight_matrix = zeros((vocab_size, 100))\n",
    "    # step vocab, store vectors using the Tokenizer's integer mapping\n",
    "    for word, i in vocab.items():\n",
    "        vector = embedding.get(word)\n",
    "        if vector is not None:\n",
    "            weight_matrix[i] = vector\n",
    "    return weight_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_embedding(filename):\n",
    "    # load embedding into memory, skip first line\n",
    "    file = open(filename, 'r')\n",
    "    lines = file.readlines()\n",
    "    file.close()\n",
    "    # create a map of words to vectors\n",
    "    embedding = dict()\n",
    "    for line in lines:\n",
    "        parts = line.split()\n",
    "        # key is string word, value is numpy array for vector\n",
    "        embedding[parts[0]] = asarray(parts[1:], dtype='float32')\n",
    "    return embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load embedding from file\n",
    "raw_embedding = load_embedding('glove.6B.100d.txt')\n",
    "# get vectors in the right order\n",
    "embedding_vectors = get_weight_matrix(raw_embedding, tokenizer.word_index)\n",
    "# create the embedding layer\n",
    "embedding_layer = Embedding(vocab_size, 100, weights=[embedding_vectors], input_length=max_length, trainable=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_5\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_3 (Embedding)      (None, 30, 100)           1074200   \n",
      "_________________________________________________________________\n",
      "conv1d_2 (Conv1D)            (None, 26, 128)           64128     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_2 (MaxPooling1 (None, 13, 128)           0         \n",
      "_________________________________________________________________\n",
      "flatten_3 (Flatten)          (None, 1664)              0         \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 1)                 1665      \n",
      "=================================================================\n",
      "Total params: 1,139,993\n",
      "Trainable params: 65,793\n",
      "Non-trainable params: 1,074,200\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# define model\n",
    "model_glove = Sequential()\n",
    "model_glove.add(embedding_layer)\n",
    "model_glove.add(Conv1D(filters=128, kernel_size=5, activation='relu'))\n",
    "model_glove.add(MaxPooling1D(pool_size=2))\n",
    "model_glove.add(Flatten())\n",
    "model_glove.add(Dense(1, activation='sigmoid'))\n",
    "print(model_glove.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      " - 1s - loss: 0.4643 - accuracy: 0.7746\n",
      "Epoch 2/10\n",
      " - 1s - loss: 0.2538 - accuracy: 0.8996\n",
      "Epoch 3/10\n",
      " - 1s - loss: 0.1478 - accuracy: 0.9530\n",
      "Epoch 4/10\n",
      " - 1s - loss: 0.0895 - accuracy: 0.9772\n",
      "Epoch 5/10\n",
      " - 1s - loss: 0.0483 - accuracy: 0.9931\n",
      "Epoch 6/10\n",
      " - 1s - loss: 0.0258 - accuracy: 0.9985\n",
      "Epoch 7/10\n",
      " - 1s - loss: 0.0173 - accuracy: 0.9989\n",
      "Epoch 8/10\n",
      " - 1s - loss: 0.0125 - accuracy: 0.9992\n",
      "Epoch 9/10\n",
      " - 1s - loss: 0.0085 - accuracy: 0.9992\n",
      "Epoch 10/10\n",
      " - 1s - loss: 0.0084 - accuracy: 0.9989\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x1a4bbbff98>"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# compile network\n",
    "model_glove.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "# fit network\n",
    "model_glove.fit(Xtrain, y_train, epochs=10, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 90.760297\n"
     ]
    }
   ],
   "source": [
    "# evaluation\n",
    "loss, acc = model_glove.evaluate(Xtest, y_test, verbose=0)\n",
    "print('Test Accuracy: %f' % (acc*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import fasttext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write the test file.\n",
    "with open(\"./test.txt\", \"w\") as test_file_handler:\n",
    "    for X_test_entry, y_test_entry in zip(x_test, y_test):\n",
    "        line_to_write = \"__label__\" + str(y_test_entry) + \"\\t\" + str(X_test_entry) + \"\\n\"\n",
    "        try:\n",
    "            test_file_handler.write(line_to_write)\n",
    "        except:\n",
    "            print(line_to_write)\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write the train file.\n",
    "with open(\"./train.txt\", \"w\") as train_file_handler:\n",
    "    for X_train_entry, y_train_entry in zip(x_train, y_train):\n",
    "        line_to_write = \"__label__\" + str(y_train_entry) + \"\\t\" + str(X_train_entry) + \"\\n\"\n",
    "        try:\n",
    "            train_file_handler.write(line_to_write)\n",
    "        except:\n",
    "            print(line_to_write)\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = fasttext.train_supervised(input=\"./train.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N\t1894\n",
      "P@1\t0.970\n",
      "R@1\t0.970\n"
     ]
    }
   ],
   "source": [
    "def print_results(N, p, r):\n",
    "    print(\"N\\t\" + str(N))\n",
    "    print(\"P@{}\\t{:.3f}\".format(1, p))\n",
    "    print(\"R@{}\\t{:.3f}\".format(1, r))\n",
    "\n",
    "results = model.test(\"./test.txt\")\n",
    "print_results(*results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
